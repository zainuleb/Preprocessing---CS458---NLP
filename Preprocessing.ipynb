{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "corpus=open('Movies_TV.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing first line, which is unnecessary.\n",
    "res=re.sub(r'Domain.*\\n','',corpus)\n",
    "#res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Read data from the dataset and separate it into 4 columns with each row as a review submitted.\n",
    "rows=res.split('\\n')\n",
    "rows.remove(rows[-1])\n",
    "necsData=[]\n",
    "for data in rows:\n",
    "    \n",
    "    #a) Removing unwanted whitespaces\n",
    "    _,_,_,review=data.split('\\t')\n",
    "    \n",
    "    #b) Normalizing case\n",
    "    necsData.append(review.lower())  #Conversion to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Removinig stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stopWordsList=list(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as punc\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempList=[]\n",
    "stemmingList=[]\n",
    "for rev in necsData:\n",
    "    tempList=rev.split(\" \")\n",
    "    remStopWord=[word for word in tempList if word not in stopWordsList] #Removing stop words.\n",
    "    remPunctuation=[word for word in remStopWord if word not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempList=[]\n",
    "stemmingList=[]\n",
    "finalList=[]\n",
    "joinOn=\" \"\n",
    "tempSplit=[]\n",
    "\n",
    "for rev in necsData:\n",
    "    tempList=rev.split(\" \")\n",
    "    remStopWord=[word for word in tempList if word not in stopWordsList] #Removing stop words.\n",
    "    remPunctuation=[word for word in remStopWord if word not in punc]\n",
    "    for word in remPunctuation:\n",
    "        \n",
    "        \n",
    "        stemmingList.append(wl.lemmatize(ps.stem(word),'v')) #Stemming and lemmetizing words.\n",
    "    #Joining words back toghether of the review after preprocessing techniques to make it whole.\n",
    "    joinStr=joinOn.join(stemmingList)\n",
    "    #List that contains reviews after the preprocessing techniques as elements.\n",
    "    finalList.append(joinStr)\n",
    "    stemmingList=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Find all possible unigrams, bigrams and trigrams for the first review (its computationally expensive)\n",
    "\n",
    "from nltk import ngrams\n",
    "tempStr=finalList[0] #Selecting first review\n",
    "ngramsList=tempStr.split(\" \") #List of words of the first review.\n",
    "unigrams=list(ngrams(ngramsList,1))\n",
    "bigrams=list(ngrams(ngramsList,2))\n",
    "trigrams=list(ngrams(ngramsList,3))\n",
    "#print(\"unigrams\", unigrams)\n",
    "#print(\"bigrams\", bigrams)\n",
    "#print(\"trigrams\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Find the probabilities of unigrams, bigrams and trigrams.\n",
    "#tempStr contains first review.\n",
    "\n",
    "unigram_freq=[unigrams.count(x)/len(set(tempStr)) for x in unigrams]\n",
    "#unigram_freq\n",
    "\n",
    "bigram_freq=[bigrams.count(x)/ngramsList.count(x[0]) for x in bigrams]\n",
    "#bigram_freq\n",
    "\n",
    "trigram_freq=[trigrams.count(x)/bigrams.count(x[:2]) for x in trigrams]\n",
    "#trigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) Provide the following information about reivews;\n",
    "emptySpace = \" \"\n",
    "tempStr2 = emptySpace.join(finalList)\n",
    "\n",
    "tokkenList = tempStr2.split(\" \")\n",
    "\n",
    "#a) total tokens\n",
    "#len(tokkenList)\n",
    "\n",
    "#b) Vocabulary (unique tokens) before preprocessing\n",
    "#len(set(tokkenList)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempStr3=emptySpace.join(necsData)\n",
    "uniqueTokkensAft=tempStr3.split(\" \")\n",
    "len(uniqueTokkensAft)\n",
    "len(set(uniqueTokkensAft)) #Number of unique tokkens before processing\n",
    "\n",
    "wordList=[]\n",
    "counter=0\n",
    "lengthRev=0\n",
    "for rev in necsData:\n",
    "    wordList=rev.split(\" \") \n",
    "    lengthRev=lengthRev+len(wordList) #Total number of words in a review.\n",
    "    counter=counter+1         #Total reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Average length of a review\n",
    "avgLengthRev=lengthRev/counter\n",
    "avgLengthRev\n",
    "\n",
    "wordList=[]\n",
    "counter=0\n",
    "wordLength=0\n",
    "tWords=0\n",
    "avg1Rev=0\n",
    "for rev in necsData:\n",
    "    wordList=rev.split(\" \")\n",
    "    tWords=len(wordList)   #Number of words in a single review.\n",
    "    counter=counter+1\n",
    "    for x in wordList:\n",
    "        wordLength=wordLength+len(x)\n",
    "\n",
    "    avg1Rev=avg1Rev+(wordLength/tWords) \n",
    "    wordLength=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Average length of tokens within a review\n",
    "totalAvg=avg1Rev/counter\n",
    "#totalAvg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
